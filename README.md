# ScanAnnotateSummarize
An application that performs extractive summarization on text extracted from images using OCR. 

## Project Description
This project deals with designing a system that performs *extractive summarization* on text extracted from images using *optical character recognition*. The text extracted is subjected to various extractive summarization algorithms and the accuracy of the generated summary is calculated using different metrics. 

## Background 
Text summarization is the process of identifying the most relevant information and discarding the unnecessary and the irrelevant information. Summarization has two approaches, Abstractive text summarization and Extractive text summarization. Extractive summarization involves the
extraction of the most important sentences from a given document whereas abstractive text summarization involves rephrasing and restructuring of the sentence phrases to generate a summary of the given document without copying verbatim most salient sentences from text. 

## Methodology 
The methodology is comprised of five tasks, 
1. Generating the dataset 
2. Extraction of text from input image - using tesseract and opencv 
3. Text preprocessing using nltk toolkit and regular expressions 
4. Application of extractive summarization algorithms like Word Frequency, TextRank, LexRank, TF-IDF, Luhn and LSA
5. Evaluating the generating summaries based on the Rouge metric using the abstractive summary generated by Google's T5 Model as reference

| ![](https://user-images.githubusercontent.com/110343068/204125269-542035f6-a839-4f36-9ee3-298221b71e8f.JPG) |
| :--: |
| *Overview of the process* |

## Dataset details
The dataset is comprised of images taken from textbook. The textbook in pdf format is converted by extracting the pdf as images in jpg/jpeg format. The images are then annotated to focus on those parts of pure text for extraction and summarization. The image files are annotated by drawing bounding boxes. The bounding boxes are named as BB1, BB2, ...and so on. The co-ordinates of the bounding boxes are saved as an xml file. The image along with its xml file forms the dataset.

| ![Design_1](https://user-images.githubusercontent.com/110343068/204125254-5a6d932b-c47e-4af2-8ee0-dae44f59292f.JPG) |
| :--: |
| *Preparation of the dataset* |

## Tools 
**LabelImg** - LabelImg is a graphical image annotation tool. It is written in Python and uses Qt for its graphical interface. Annotations are saved as XML files in PASCAL VOC format, the format used by ImageNet. This tool is used to annotate the images by drawing bounding boxes across the textual content in the images. 

| ![LabelImg](https://user-images.githubusercontent.com/110343068/204125282-6b11f9be-c707-4c6e-8862-22537d2bc403.JPG) |
| :--: |
| *Annotation of text content using LabelImg* |
